{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5250ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edaf2e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14b5a6830d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b316a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.CenterCrop(16),\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "#torchvision.transforms.Normalize(\n",
    "#                                 (0.1307,), (0.3081,))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.CenterCrop(16),\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7bc0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 16, 16])\n",
      "torch.Size([1000])\n",
      "tensor([4, 1, 7, 3, 9, 5, 5, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "print(example_targets.shape)\n",
    "print(example_targets[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b9580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 1.0000, 1.0000,\n",
      "         1.0000, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 1.0000, 0.8863, 0.1137,\n",
      "         1.0000, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8863, 1.0000, 0.0000, 0.0000,\n",
      "         0.7765, 1.0000, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5529, 0.0000, 0.0000,\n",
      "         0.6667, 1.0000, 0.4471, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4471, 1.0000, 0.1137, 0.0000, 0.0000,\n",
      "         1.0000, 1.0000, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 1.0000, 0.0000, 0.0000, 0.4471,\n",
      "         1.0000, 1.0000, 0.7765, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 1.0000, 0.0000, 0.0000, 0.7765,\n",
      "         1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 1.0000, 0.2235, 0.0000, 1.0000,\n",
      "         1.0000, 0.6667, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 1.0000, 0.8863, 0.3373, 1.0000,\n",
      "         0.5529, 1.0000, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 1.0000, 1.0000, 1.0000,\n",
      "         0.6667, 1.0000, 0.5529, 0.3373, 0.6667, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4471, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 0.7765, 0.1137, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.6667, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 1.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 1.0000, 1.0000,\n",
      "         0.7765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 1.0000, 1.0000,\n",
      "         0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 1.0000, 1.0000, 0.6667,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 1.0000, 1.0000,\n",
      "         1.0000, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.5529, 1.0000, 0.8863, 0.1137, 1.0000, 0.6667,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.8863, 1.0000, 0.0000, 0.0000, 0.7765, 1.0000, 0.1137, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.5529, 0.0000, 0.0000, 0.6667, 1.0000, 0.4471, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4471, 1.0000, 0.1137, 0.0000,\n",
      "         0.0000, 1.0000, 1.0000, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6667, 1.0000, 0.0000, 0.0000, 0.4471, 1.0000,\n",
      "         1.0000, 0.7765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.7765, 1.0000, 0.0000, 0.0000, 0.7765, 1.0000, 1.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529,\n",
      "         1.0000, 0.2235, 0.0000, 1.0000, 1.0000, 0.6667, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1137, 1.0000, 0.8863,\n",
      "         0.3373, 1.0000, 0.5529, 1.0000, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 1.0000, 1.0000, 1.0000,\n",
      "         0.6667, 1.0000, 0.5529, 0.3373, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4471, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.7765, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.6667, 0.1137, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1137, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5529, 1.0000,\n",
      "         1.0000, 0.7765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 1.0000, 1.0000, 0.2235,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1137, 1.0000, 1.0000, 0.6667, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(example_data[1][0])\n",
    "print(example_data[1][0].view(-1,16*16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72591244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56683d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f02328bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.fc1 = nn.Linear(28*28, 16)\n",
    "        self.fc1 = nn.Linear(16*16, 8)\n",
    "        #self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,16*16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n",
    "        #return torch.argmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daa7fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03971fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f6d5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(network.state_dict(), './model.pth')\n",
    "      torch.save(optimizer.state_dict(), './optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a74e3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa70c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-d914daa2036d>:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3348, Accuracy: 1058/10000 (11%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304919\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.283166\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.295948\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.269189\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.188981\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.181474\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.192395\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.160404\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.134141\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.165720\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.158950\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.066383\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.071828\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.024808\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.951607\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.948059\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.924226\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.884079\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.842421\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.798001\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.816162\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.749867\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.667985\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.607937\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.547300\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.528171\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.418554\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.489280\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.448493\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.392012\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.379367\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.341670\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.258618\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.350462\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.259464\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.261463\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.062339\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.231731\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.101179\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.126199\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.186764\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.934641\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.071454\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.060440\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.031471\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.980566\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.059243\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.860795\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.937336\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.901361\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.837144\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.740938\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.803416\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.880683\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.806292\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.746036\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.847352\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.832748\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.825430\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.897153\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.803219\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.694554\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.717021\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.727053\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.931228\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.653540\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.672430\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.756027\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.849454\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.943894\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.715527\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.750903\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.720006\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.680702\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.561257\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.559934\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.744813\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.621198\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.725630\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.605158\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.632000\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.506475\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.588226\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.635266\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.621139\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.704177\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.723222\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.654616\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.468232\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.741290\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.395317\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.810680\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.747511\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.908830\n",
      "\n",
      "Test set: Avg. loss: 0.5970, Accuracy: 8170/10000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.657670\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.571144\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.654748\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.802166\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.501756\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.596430\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.504198\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.573897\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.831627\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.746961\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.574241\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.495471\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.548093\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.796379\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.785739\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.499521\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.850831\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.576085\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.567645\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.560280\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.863054\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.469871\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.374325\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.687392\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.596930\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.623366\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.435880\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.546860\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.499435\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.436397\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.576543\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.469520\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.355243\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.476778\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.577261\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.695343\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.723943\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.757465\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.333694\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.750592\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.468119\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.530720\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.352039\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.411322\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.638976\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.532591\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.557807\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.399393\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.458401\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.579644\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.577157\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.577129\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.503860\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.442102\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.474499\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.506865\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.497830\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.582560\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.565040\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.372364\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.510444\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.495581\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.603814\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.299136\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.430394\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.594230\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.355650\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.454394\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.559097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.575513\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.538382\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.666324\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.316027\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.522375\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.498113\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.351359\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.789061\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.627599\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.586164\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.406997\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.380032\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.620651\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.484537\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.332312\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.599497\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.561336\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.412646\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.430517\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.542545\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.451550\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.493902\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.559249\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.446700\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.408319\n",
      "\n",
      "Test set: Avg. loss: 0.4600, Accuracy: 8631/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.327983\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.569563\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.330253\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.297077\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.445835\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.518997\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.372550\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.409440\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.545259\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.308643\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.444178\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.923365\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.427847\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.458459\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.535909\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.421714\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.456790\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.577673\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.471343\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.377973\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.509930\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.521974\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.367738\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.483782\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.411738\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.385207\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.452787\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.388215\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.473725\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.379356\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.273627\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.426319\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.280172\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.428540\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.479961\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.383699\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.414721\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.486691\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.651231\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.431947\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.361141\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.499532\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.375350\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.485190\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.304035\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.277410\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.553124\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.332130\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.422192\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.288143\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.573694\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.411696\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.240183\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.194928\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.388155\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.450483\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.691340\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.291726\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.593132\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.440857\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.381274\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.519362\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.499647\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.352731\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.597026\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.397200\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.470660\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.422825\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.388401\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.465789\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.401039\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.699709\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.424609\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.616854\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.353907\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.435775\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.436491\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.532441\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.440475\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.517880\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.254488\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.559363\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.296151\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.381266\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.326067\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.433830\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.486844\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.480986\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.650105\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.502955\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.221944\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.357079\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.448962\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.393557\n",
      "\n",
      "Test set: Avg. loss: 0.4120, Accuracy: 8798/10000 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9389ca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's state_dict\n",
      "fc1.weight \t torch.Size([8, 256])\n",
      "tensor([[-0.0626,  0.0485, -0.0006,  ...,  0.0224,  0.0051,  0.0434],\n",
      "        [ 0.1667,  0.2019,  0.1502,  ...,  0.2417,  0.2129,  0.0817],\n",
      "        [-0.0457, -0.0439, -0.0237,  ...,  0.0085, -0.0715, -0.0431],\n",
      "        ...,\n",
      "        [ 0.0373,  0.0114,  0.0380,  ...,  0.0834,  0.1180,  0.1187],\n",
      "        [ 0.0135, -0.0090, -0.0727,  ..., -0.0897, -0.1196, -0.0666],\n",
      "        [ 0.0320,  0.0438,  0.0891,  ...,  0.0579,  0.0855,  0.1291]])\n",
      "fc1.bias \t torch.Size([8])\n",
      "tensor([-0.0584, -0.0844,  0.1476,  0.0842,  0.0808,  0.1961,  0.3523,  0.0644])\n",
      "fc3.weight \t torch.Size([10, 8])\n",
      "tensor([[-0.2774, -0.1641, -0.7131,  0.8463, -0.7223, -0.4398,  0.6920,  0.7079],\n",
      "        [-0.4782, -0.5926,  0.4943, -0.2513, -0.6075,  1.3375, -0.5672,  0.1815],\n",
      "        [ 0.5671,  0.5567, -0.5974, -0.0513, -0.3223,  0.5959, -0.3678,  0.6789],\n",
      "        [-0.3303,  1.4414,  0.4949, -0.2989, -0.6434, -0.3057, -0.0991, -0.2645],\n",
      "        [-0.7042, -0.2126, -0.3400, -0.5201,  1.5041, -0.3411, -0.1948,  0.0323],\n",
      "        [-0.5693, -0.0835,  0.3493,  1.0251, -0.2627, -0.5562, -0.0078, -0.5971],\n",
      "        [ 1.1865, -0.0711, -0.4149,  0.1623,  0.3864, -0.6050, -0.9598, -0.3535],\n",
      "        [ 0.2263, -0.5349,  0.4532, -1.1000, -0.4117,  0.2268,  1.2646,  0.2814],\n",
      "        [-0.0206,  0.0565,  0.7294,  0.4131,  0.0538, -0.0955, -0.5293, -0.1556],\n",
      "        [ 0.1070,  0.0205,  0.2039, -0.4421,  0.4381, -0.3221,  0.7340, -0.6251]])\n",
      "fc3.bias \t torch.Size([10])\n",
      "tensor([-0.1642,  0.4245, -0.0955, -0.0124, -0.2633,  0.5998,  0.0167,  0.1437,\n",
      "        -0.0121,  0.0605])\n"
     ]
    }
   ],
   "source": [
    "# print model's state_dict\n",
    "print(\"model's state_dict\")\n",
    "for param_tensor in network.state_dict():\n",
    "    print(param_tensor, \"\\t\", network.state_dict()[param_tensor].size())\n",
    "    print(network.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85b5b8",
   "metadata": {},
   "source": [
    "## TODO: store the test data in .txt file; store the parameters in .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edfe80",
   "metadata": {},
   "source": [
    "### Already get network.state_dict(), containing all parameters.\n",
    "### Already get test_loader, can print test data in (1,256) (like example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c7023c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-11.8112,  -2.0654,  -3.6321,  -3.8821,  -1.8789,  -6.3563,  -6.3382,\n",
      "          -2.4117,  -1.7280,  -0.9098]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[9]])\n",
      "torch.return_types.max(\n",
      "values=tensor([[-0.9098]]),\n",
      "indices=tensor([[9]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-759eb957afec>:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "output = network(example_data[1][0])\n",
    "print(output)\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "print(pred)\n",
    "print(output.data.max(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ecb06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
